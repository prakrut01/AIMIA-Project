{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c91f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: 0\n",
      "GPU name: NVIDIA RTX A5000\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.00 GB\n",
      "Max memory allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "\n",
    "# Get current GPU information\n",
    "current_gpu = torch.cuda.current_device()\n",
    "print(f\"Using GPU: {current_gpu}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "\n",
    "# Memory usage for current GPU\n",
    "allocated = torch.cuda.memory_allocated(current_gpu) / 1024**3  # Convert to GB\n",
    "reserved = torch.cuda.memory_reserved(current_gpu) / 1024**3    # Convert to GB\n",
    "max_allocated = torch.cuda.max_memory_allocated(current_gpu) / 1024**3\n",
    "\n",
    "print(f\"Memory allocated: {allocated:.2f} GB\")\n",
    "print(f\"Memory reserved: {reserved:.2f} GB\") \n",
    "print(f\"Max memory allocated: {max_allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8965407a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Fallback if torchvision not available\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from torchvision import transforms\n",
    "except ImportError:\n",
    "    # Fallback if torchvision not available\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torchvision\"])\n",
    "    from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"Installing required libraries...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"timm\", \"tqdm\"])\n",
    "    import timm\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b80e73a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstalling timm library...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/timm/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __version__\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     is_scriptable \u001b[38;5;28;01mas\u001b[39;00m is_scriptable,\n\u001b[1;32m      4\u001b[0m     is_exportable \u001b[38;5;28;01mas\u001b[39;00m is_exportable,\n\u001b[1;32m      5\u001b[0m     set_scriptable \u001b[38;5;28;01mas\u001b[39;00m set_scriptable,\n\u001b[1;32m      6\u001b[0m     set_exportable \u001b[38;5;28;01mas\u001b[39;00m set_exportable,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     create_model \u001b[38;5;28;01mas\u001b[39;00m create_model,\n\u001b[1;32m     10\u001b[0m     list_models \u001b[38;5;28;01mas\u001b[39;00m list_models,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     get_pretrained_cfg_value \u001b[38;5;28;01mas\u001b[39;00m get_pretrained_cfg_value,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/timm/layers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     create_feature_extractor,\n\u001b[1;32m      3\u001b[0m     get_graph_node_names,\n\u001b[1;32m      4\u001b[0m     register_notrace_function,\n\u001b[1;32m      5\u001b[0m     register_notrace_module,\n\u001b[1;32m      6\u001b[0m     is_notrace_module,\n\u001b[1;32m      7\u001b[0m     is_notrace_function,\n\u001b[1;32m      8\u001b[0m     get_notrace_modules,\n\u001b[1;32m      9\u001b[0m     get_notrace_functions,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptive_avgmax_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     adaptive_avgmax_pool2d,\n\u001b[1;32m     14\u001b[0m     select_adaptive_pool2d,\n\u001b[1;32m     15\u001b[0m     AdaptiveAvgMaxPool2d,\n\u001b[1;32m     16\u001b[0m     SelectAdaptivePool2d,\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/timm/layers/_fx.py:8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# NOTE we wrap torchvision fns to use timm leaf / no trace definitions\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_feature_extractor \u001b[38;5;28;01mas\u001b[39;00m _create_feature_extractor\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_graph_node_names \u001b[38;5;28;01mas\u001b[39;00m _get_graph_node_names\n\u001b[1;32m     10\u001b[0m     has_fx_feature_extraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    print(\"Installing timm library...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"timm\"])\n",
    "    import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required libraries...\n",
      "Collecting timm\n",
      "  Using cached timm-1.0.22-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: tqdm in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from timm) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from timm) (0.20.1+cu121)\n",
      "Requirement already satisfied: pyyaml in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from timm) (0.30.2)\n",
      "Collecting safetensors (from timm)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data1/home/prakrutp/miniconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.10.5)\n",
      "Using cached timm-1.0.22-py3-none-any.whl (2.5 MB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors, timm\n",
      "Successfully installed safetensors-0.6.2 timm-1.0.22\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mcheck_call([sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Vision Transformer Configuration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/timm/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __version__\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     is_scriptable \u001b[38;5;28;01mas\u001b[39;00m is_scriptable,\n\u001b[1;32m      4\u001b[0m     is_exportable \u001b[38;5;28;01mas\u001b[39;00m is_exportable,\n\u001b[1;32m      5\u001b[0m     set_scriptable \u001b[38;5;28;01mas\u001b[39;00m set_scriptable,\n\u001b[1;32m      6\u001b[0m     set_exportable \u001b[38;5;28;01mas\u001b[39;00m set_exportable,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     create_model \u001b[38;5;28;01mas\u001b[39;00m create_model,\n\u001b[1;32m     10\u001b[0m     list_models \u001b[38;5;28;01mas\u001b[39;00m list_models,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     get_pretrained_cfg_value \u001b[38;5;28;01mas\u001b[39;00m get_pretrained_cfg_value,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/timm/layers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     create_feature_extractor,\n\u001b[1;32m      3\u001b[0m     get_graph_node_names,\n\u001b[1;32m      4\u001b[0m     register_notrace_function,\n\u001b[1;32m      5\u001b[0m     register_notrace_module,\n\u001b[1;32m      6\u001b[0m     is_notrace_module,\n\u001b[1;32m      7\u001b[0m     is_notrace_function,\n\u001b[1;32m      8\u001b[0m     get_notrace_modules,\n\u001b[1;32m      9\u001b[0m     get_notrace_functions,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptive_avgmax_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     adaptive_avgmax_pool2d,\n\u001b[1;32m     14\u001b[0m     select_adaptive_pool2d,\n\u001b[1;32m     15\u001b[0m     AdaptiveAvgMaxPool2d,\n\u001b[1;32m     16\u001b[0m     SelectAdaptivePool2d,\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/timm/layers/_fx.py:8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# NOTE we wrap torchvision fns to use timm leaf / no trace definitions\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_feature_extractor \u001b[38;5;28;01mas\u001b[39;00m _create_feature_extractor\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_graph_node_names \u001b[38;5;28;01mas\u001b[39;00m _get_graph_node_names\n\u001b[1;32m     10\u001b[0m     has_fx_feature_extraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Install required packages\n",
    "try:\n",
    "    import timm\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"Installing required libraries...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"timm\", \"tqdm\"])\n",
    "    import timm\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Vision Transformer Configuration\n",
    "class Config:\n",
    "    data_dir = \"/data1/home/prakrutp/medical_imaging/dataset\"\n",
    "    csv_path = \"/data1/home/prakrutp/medical_imaging/dataset/Data_Entry_2017.csv\"\n",
    "    image_size = 224  # ViT standard input size\n",
    "    batch_size = 16   # Reduced for ViT memory requirements\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4  # Lower LR for ViT\n",
    "    num_classes = 14\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Vision Transformer specific\n",
    "    model_name = \"vit_base_patch16_224\"\n",
    "    use_pretrained = True\n",
    "    \n",
    "    # Class imbalance handling\n",
    "    use_class_weights = True\n",
    "    \n",
    "    # Split files\n",
    "    test_list_file = \"test_list.txt\"\n",
    "    train_val_list_file = \"train_val_list.txt\"\n",
    "    \n",
    "    # Checkpoint settings\n",
    "    checkpoint_dir = \"checkpoints_vit\"\n",
    "    checkpoint_interval = 5\n",
    "    resume_training = False\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ==================== CUSTOM TRANSFORMS USING PILLOW ====================\n",
    "class CustomTransform:\n",
    "    \"\"\"Custom transform class using only PIL to avoid torchvision issues\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(tensor):\n",
    "        \"\"\"Normalize tensor with ImageNet stats\"\"\"\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "        return (tensor - mean) / std\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_tensor(pil_image):\n",
    "        \"\"\"Convert PIL image to tensor\"\"\"\n",
    "        return torch.from_numpy(np.array(pil_image)).float().permute(2, 0, 1) / 255.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def resize(pil_image, size):\n",
    "        \"\"\"Resize PIL image\"\"\"\n",
    "        return pil_image.resize((size, size), Image.BILINEAR)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_horizontal_flip(pil_image, p=0.5):\n",
    "        \"\"\"Random horizontal flip\"\"\"\n",
    "        if random.random() < p:\n",
    "            return pil_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return pil_image\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_rotation(pil_image, degrees=15):\n",
    "        \"\"\"Random rotation\"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            angle = random.uniform(-degrees, degrees)\n",
    "            return pil_image.rotate(angle, Image.BILINEAR)\n",
    "        return pil_image\n",
    "    \n",
    "    @staticmethod\n",
    "    def color_jitter(pil_image, brightness=0.2, contrast=0.2):\n",
    "        \"\"\"Color jitter using PIL\"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            # Brightness\n",
    "            factor = 1 + random.uniform(-brightness, brightness)\n",
    "            pil_image = ImageEnhance.Brightness(pil_image).enhance(factor)\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            # Contrast\n",
    "            factor = 1 + random.uniform(-contrast, contrast)\n",
    "            pil_image = ImageEnhance.Contrast(pil_image).enhance(factor)\n",
    "        \n",
    "        return pil_image\n",
    "\n",
    "class TrainTransform:\n",
    "    def __init__(self, size=224):\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, pil_image):\n",
    "        # Apply augmentations\n",
    "        pil_image = CustomTransform.resize(pil_image, self.size)\n",
    "        pil_image = CustomTransform.random_horizontal_flip(pil_image, p=0.5)\n",
    "        pil_image = CustomTransform.random_rotation(pil_image, degrees=15)\n",
    "        pil_image = CustomTransform.color_jitter(pil_image, brightness=0.2, contrast=0.2)\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        tensor = CustomTransform.to_tensor(pil_image)\n",
    "        tensor = CustomTransform.normalize(tensor)\n",
    "        return tensor\n",
    "\n",
    "class ValTransform:\n",
    "    def __init__(self, size=224):\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, pil_image):\n",
    "        # Only resize for validation\n",
    "        pil_image = CustomTransform.resize(pil_image, self.size)\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        tensor = CustomTransform.to_tensor(pil_image)\n",
    "        tensor = CustomTransform.normalize(tensor)\n",
    "        return tensor\n",
    "\n",
    "def get_vit_transforms():\n",
    "    \"\"\"Get train and validation transforms\"\"\"\n",
    "    return TrainTransform(size=Config.image_size), ValTransform(size=Config.image_size)\n",
    "\n",
    "# ==================== DATASET CLASS ====================\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_dir, image_list=None, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.disease_classes = [\n",
    "            'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', \n",
    "            'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', \n",
    "            'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', \n",
    "            'Pleural_Thickening', 'Hernia'\n",
    "        ]\n",
    "        \n",
    "        self._create_label_columns()\n",
    "        \n",
    "        if image_list is not None:\n",
    "            self.data_frame = self.df[self.df['Image Index'].isin(image_list)].reset_index(drop=True)\n",
    "        else:\n",
    "            self.data_frame = self.df\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.data_frame)} images\")\n",
    "        \n",
    "    def _create_label_columns(self):\n",
    "        for disease in self.disease_classes:\n",
    "            self.df.loc[:, disease] = self.df['Finding Labels'].apply(\n",
    "                lambda x: 1 if disease in x else 0\n",
    "            )\n",
    "    \n",
    "    def _find_image_path(self, img_name):\n",
    "        for i in range(1, 13):\n",
    "            folder_name = f\"images_{i:03d}\"\n",
    "            possible_path = os.path.join(self.base_dir, folder_name, \"images\", img_name)\n",
    "            if os.path.exists(possible_path):\n",
    "                return possible_path\n",
    "        return os.path.join(self.base_dir, \"images_001\", \"images\", img_name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data_frame.iloc[idx]['Image Index']\n",
    "        img_path = self._find_image_path(img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load image {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (Config.image_size, Config.image_size), color='black')\n",
    "        \n",
    "        labels = []\n",
    "        for disease in self.disease_classes:\n",
    "            labels.append(self.data_frame.iloc[idx][disease])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.FloatTensor(labels)\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        distribution = {}\n",
    "        for disease in self.disease_classes:\n",
    "            count = self.data_frame[disease].sum()\n",
    "            percentage = (count / len(self.data_frame)) * 100\n",
    "            distribution[disease] = {'count': count, 'percentage': percentage}\n",
    "        return distribution\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "def load_split_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        image_names = [line.strip() for line in f.readlines()]\n",
    "    return image_names\n",
    "\n",
    "def create_datasets_from_splits(csv_file, base_dir, test_list_file, train_val_list_file, transform=None):\n",
    "    test_images = load_split_file(test_list_file)\n",
    "    train_val_images = load_split_file(train_val_list_file)\n",
    "    \n",
    "    print(f\"Test images: {len(test_images)}\")\n",
    "    print(f\"Train+Val images: {len(train_val_images)}\")\n",
    "    \n",
    "    train_images, val_images = train_test_split(\n",
    "        train_val_images, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train images: {len(train_images)}\")\n",
    "    print(f\"Validation images: {len(val_images)}\")\n",
    "    \n",
    "    train_dataset = ChestXrayDataset(\n",
    "        csv_file=csv_file,\n",
    "        base_dir=base_dir,\n",
    "        image_list=train_images,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = ChestXrayDataset(\n",
    "        csv_file=csv_file,\n",
    "        base_dir=base_dir,\n",
    "        image_list=val_images,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = ChestXrayDataset(\n",
    "        csv_file=csv_file,\n",
    "        base_dir=base_dir,\n",
    "        image_list=test_images,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# ==================== VISION TRANSFORMER MODEL ====================\n",
    "class VisionTransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes=14, model_name=\"vit_base_patch16_224\"):\n",
    "        super(VisionTransformerModel, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Vision Transformer\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=Config.use_pretrained,\n",
    "            num_classes=0  # Remove default classifier\n",
    "        )\n",
    "        \n",
    "        # Get feature dimension\n",
    "        if \"base\" in model_name:\n",
    "            self.feature_dim = 768\n",
    "        elif \"large\" in model_name:\n",
    "            self.feature_dim = 1024\n",
    "        else:\n",
    "            self.feature_dim = self.backbone.num_features\n",
    "        \n",
    "        # Custom classifier for multi-label classification\n",
    "        self.classifier = nn.Linear(self.feature_dim, num_classes)\n",
    "        \n",
    "        # Sigmoid for multi-label classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        print(f\"Initialized {model_name} with {self.feature_dim} feature dimensions\")\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from Vision Transformer\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply dropout\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(features)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ==================== LOSS FUNCTION ====================\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    \"\"\"Weighted BCE Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, class_weights=None):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        batch_size, num_classes = targets.shape\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(num_classes):\n",
    "            class_output = outputs[:, i]\n",
    "            class_target = targets[:, i]\n",
    "            \n",
    "            # Binary cross entropy for each class\n",
    "            class_loss = nn.BCELoss()(class_output, class_target)\n",
    "            \n",
    "            # Apply class weights if provided\n",
    "            if self.class_weights is not None:\n",
    "                class_loss = class_loss * self.class_weights[i]\n",
    "            \n",
    "            loss += class_loss\n",
    "        \n",
    "        return loss / num_classes\n",
    "\n",
    "# ==================== CHECKPOINT FUNCTIONS ====================\n",
    "def save_checkpoint(epoch, model, optimizer, train_losses, val_losses, val_auc_scores, best_auc, is_best=False):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_auc_scores': val_auc_scores,\n",
    "        'best_auc': best_auc,\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    filename = f\"checkpoint_epoch_{epoch:03d}.pth\"\n",
    "    checkpoint_path = os.path.join(Config.checkpoint_dir, filename)\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    if is_best:\n",
    "        best_path = os.path.join(Config.checkpoint_dir, \"best_model.pth\")\n",
    "        torch.save(checkpoint, best_path)\n",
    "    \n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def find_latest_checkpoint():\n",
    "    checkpoint_files = [f for f in os.listdir(Config.checkpoint_dir) if f.startswith('checkpoint_epoch_')]\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    epochs = [int(f.split('_')[2].split('.')[0]) for f in checkpoint_files]\n",
    "    latest_epoch = max(epochs)\n",
    "    return os.path.join(Config.checkpoint_dir, f\"checkpoint_epoch_{latest_epoch:03d}.pth\")\n",
    "\n",
    "# ==================== CLASS WEIGHTS CALCULATION ====================\n",
    "def calculate_class_weights(dataset):\n",
    "    \"\"\"Calculate class weights for handling imbalance\"\"\"\n",
    "    print(\"Calculating class weights...\")\n",
    "    \n",
    "    # Use dataframe for fast calculation\n",
    "    class_counts = np.zeros(Config.num_classes)\n",
    "    \n",
    "    for i, disease in enumerate(dataset.disease_classes):\n",
    "        class_counts[i] = dataset.data_frame[disease].sum()\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    class_weights = total_samples / (Config.num_classes * np.maximum(class_counts, 1))\n",
    "    class_weights = class_weights / np.sum(class_weights) * Config.num_classes\n",
    "    \n",
    "    print(\"Class weights calculated:\")\n",
    "    for i, disease in enumerate(dataset.disease_classes):\n",
    "        print(f\"  {disease}: {class_weights[i]:.2f} (count: {class_counts[i]})\")\n",
    "    \n",
    "    return torch.FloatTensor(class_weights).to(Config.device)\n",
    "\n",
    "# ==================== TRAINING FUNCTION ====================\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, start_epoch=0):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_auc_scores = []\n",
    "    best_auc = 0.0\n",
    "    patience = 15\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_pbar):\n",
    "            inputs = inputs.to(Config.device)\n",
    "            labels = labels.to(Config.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for ViT stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs = inputs.to(Config.device)\n",
    "                labels = labels.to(Config.device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                all_preds.append(outputs.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        \n",
    "        auc_scores = []\n",
    "        for i in range(Config.num_classes):\n",
    "            try:\n",
    "                if np.sum(all_labels[:, i]) > 0:\n",
    "                    auc = roc_auc_score(all_labels[:, i], all_preds[:, i])\n",
    "                    auc_scores.append(auc)\n",
    "                else:\n",
    "                    auc_scores.append(0.0)\n",
    "            except:\n",
    "                auc_scores.append(0.0)\n",
    "        \n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        val_auc_scores.append(mean_auc)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        print(f'Validation AUC: {mean_auc:.4f}, Learning Rate: {current_lr:.6f}')\n",
    "        \n",
    "        # Print top diseases by AUC\n",
    "        sorted_auc = sorted(zip(train_loader.dataset.disease_classes, auc_scores), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "        print('Top 5 diseases by AUC:')\n",
    "        for disease, auc in sorted_auc[:5]:\n",
    "            print(f'  {disease}: {auc:.4f}')\n",
    "        \n",
    "        # Early stopping and checkpoint logic\n",
    "        is_best = False\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            epochs_no_improve = 0\n",
    "            is_best = True\n",
    "            print(f'New best AUC: {best_auc:.4f}')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f'No improvement for {epochs_no_improve} epochs')\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % Config.checkpoint_interval == 0 or is_best:\n",
    "            save_checkpoint(epoch+1, model, optimizer, train_losses, val_losses, val_auc_scores, best_auc, is_best)\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    return model, train_losses, val_losses, val_auc_scores\n",
    "\n",
    "# ==================== MAIN TRAINING FUNCTION ====================\n",
    "def main_vit():\n",
    "    print(\"Setting up VISION TRANSFORMER model...\")\n",
    "    print(f\"Using device: {Config.device}\")\n",
    "    print(f\"Model: {Config.model_name}\")\n",
    "    \n",
    "    # Check GPU info\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Create transforms using our custom implementation\n",
    "    train_transform, val_transform = get_vit_transforms()\n",
    "    \n",
    "    # Create datasets using existing functions\n",
    "    test_list_path = os.path.join(Config.data_dir, Config.test_list_file)\n",
    "    train_val_list_path = os.path.join(Config.data_dir, Config.train_val_list_file)\n",
    "    csv_path = os.path.join(Config.data_dir, Config.csv_path)\n",
    "    \n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets_from_splits(\n",
    "        csv_file=csv_path,\n",
    "        base_dir=Config.data_dir,\n",
    "        test_list_file=test_list_path,\n",
    "        train_val_list_file=train_val_list_path,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset.transform = val_transform\n",
    "    test_dataset.transform = val_transform\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = None\n",
    "    if Config.use_class_weights:\n",
    "        class_weights = calculate_class_weights(train_dataset)\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=Config.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=Config.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=Config.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize Vision Transformer\n",
    "    print(f\"Initializing {Config.model_name}...\")\n",
    "    model = VisionTransformerModel(\n",
    "        num_classes=Config.num_classes, \n",
    "        model_name=Config.model_name\n",
    "    )\n",
    "    model = model.to(Config.device)\n",
    "    \n",
    "    # Optimizer with weight decay (important for ViT)\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=Config.learning_rate,\n",
    "        weight_decay=0.05,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = WeightedBCELoss(class_weights=class_weights)\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\nStarting Vision Transformer training...\")\n",
    "    model, train_losses, val_losses, val_auc_scores = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        Config.num_epochs\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal evaluation on test set...\")\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "            inputs = inputs.to(Config.device)\n",
    "            outputs = model(inputs)\n",
    "            test_preds.append(outputs.cpu().numpy())\n",
    "            test_labels.append(labels.numpy())\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test_labels = np.concatenate(test_labels)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    test_auc_scores = []\n",
    "    for i in range(Config.num_classes):\n",
    "        try:\n",
    "            if np.sum(test_labels[:, i]) > 0:\n",
    "                auc = roc_auc_score(test_labels[:, i], test_preds[:, i])\n",
    "                test_auc_scores.append(auc)\n",
    "            else:\n",
    "                test_auc_scores.append(0.0)\n",
    "        except:\n",
    "            test_auc_scores.append(0.0)\n",
    "    \n",
    "    mean_auc = np.mean(test_auc_scores)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL TEST RESULTS (VISION TRANSFORMER)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Average AUC: {mean_auc:.4f}\")\n",
    "    print(\"\\nPer-class Test AUC:\")\n",
    "    for i, disease in enumerate(train_dataset.disease_classes):\n",
    "        print(f\"  {disease}: {test_auc_scores[i]:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = f'final_{Config.model_name}_model.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'test_auc': mean_auc,\n",
    "        'test_auc_scores': test_auc_scores,\n",
    "        'training_history': {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_auc_scores': val_auc_scores\n",
    "        }\n",
    "    }, final_model_path)\n",
    "    print(f\"\\nFinal model saved as '{final_model_path}'\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'{Config.model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_auc_scores)\n",
    "    plt.title(f'{Config.model_name} - Validation AUC Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{Config.model_name}_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VISION TRANSFORMER TRAINING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Final Test AUC: {mean_auc:.4f}\")\n",
    "    print(\"Expected improvement over ResNet-50: +4-6% AUC\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_vit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
